{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop([\"Host\", \"Link\", \"Date(ET)\", \"Time(ET)\", \"time(GMT)\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repalcing FACEBOOK to Facebook \n",
    "data.replace(to_replace='FACEBOOK', value='Facebook',inplace=True)\n",
    "# Now there are only 4 different values in \"Source\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replace_ = {}\n",
    "for index, i in enumerate(list(set(data[\"Source\"])),start=1):\n",
    "    replace_[index] = i\n",
    "    data.replace(to_replace=i, value=index,inplace=True)\n",
    "data.fillna('UNK',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(data[\"Source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('unk',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_with = ['.', '?', '/', '\\n', '(', ')','[', ']', '{', '}', '-','\"','!', '|' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_(sent):\n",
    "    for i in rep_with:\n",
    "        sent = sent.replace(i,' ').replace('$', ' ').replace(',','').replace(\"'\",'')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import num2words\n",
    "\n",
    "def n2w(text):\n",
    "    return re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,pos):\n",
    "    sent = []\n",
    "    for i in range(len(data)):\n",
    "        try:sent.append(n2w(rep_(data.iloc[i,pos])).replace('UNK', 'unk'))\n",
    "        except:print(data.iloc[i,pos])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = preprocess(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_t = [words for i in sent for words in i.split(' ') if words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(words_t))\n",
    "vocab.insert(0,'pad') # adding padding token in vocab\n",
    "# vocab.insert(1,'unk') # adding unk token in vocab\n",
    "stoi={i:j for j,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30418"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [ [stoi[j] for j in i.split(' ') if j] for i in sent ] # senteces\n",
    "y = [ [value] for value in data.loc[:,'Patient_Tag']  ] # emphases probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2765"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i) for i in x]) # maximum length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [ [x[i], y[i]] for i in range(len(x))] # combined x and y for training in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sorted(train, key = lambda c: len(c[0])) # sorted train using the len of x {helps in minibatching}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = []  # len of each example\n",
    "for i in train:\n",
    "    train_len.append(len(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove.6B.300d.txt\", \"r\") as f:\n",
    "    embed = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embed.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embed = {}\n",
    "for i in embed:\n",
    "    dict_embed[i.split(' ')[0]] = i.split(' ')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.random.normal(scale = 1e-6,size = (len(vocab), 300))\n",
    "words_found = 0\n",
    "for index, word in enumerate(vocab):\n",
    "    try:\n",
    "        weights_matrix[index] = dict_embed[word.lower()]\n",
    "        words_found += 1\n",
    "    except:pass\n",
    "print(\"% of words found :-\",words_found/len(vocab)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embed_train.pkl', \"wb\") as f:\n",
    "    pickle.dump(weights_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embed_train.pkl', \"rb\") as f:\n",
    "    weights_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if trainable:emb_layer.weight.requires_grad = True\n",
    "    else:emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,embedding, hidden_size, num_layers,directions,bidirectonal,out):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = directions\n",
    "        self.embedding, num_embeddings, embedding_dim = embedding\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers,bidirectional=bidirectonal)\n",
    "        self.linear1 = nn.Linear(hidden_size*directions, 32, bias=False)\n",
    "        self.linear2 = nn.Linear(32, 1, bias=False)\n",
    "        self.drop = nn.Dropout(p=0.8, inplace=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        out = self.embedding(inp)\n",
    "        out,_ = self.gru(out,hidden)\n",
    "        out = self.drop(out)\n",
    "        out = self.linear1(out.view(out.shape[1],out.shape[0],-1))\n",
    "#         out = self.drop(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers*self.directions, batch_size, self.hidden_size,dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed layer is trainable or no :---- True\n"
     ]
    }
   ],
   "source": [
    "embedding = create_emb_layer(torch.from_numpy(weights_matrix), trainable=True)\n",
    "print(\"embed layer is trainable or no :----\",embedding[0].weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(embedding,hidden_size=256,num_layers=1,directions=2,bidirectonal=True,out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(30418, 300, padding_idx=0)\n",
       "  (gru): GRU(300, 256, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=32, bias=False)\n",
       "  (linear2): Linear(in_features=32, out_features=1, bias=False)\n",
       "  (drop): Dropout(p=0.8, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "# x = pad_sequence([torch.tensor(i) for i in x])\n",
    "# y = pad_sequence([torch.tensor(i) for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = random.sample([i[0] for i in train],len(train))\n",
    "y_ = random.sample([i[1] for i in train],len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_[:1000]\n",
    "y = y_[:1000]\n",
    "\n",
    "x_test = x_[1000:]\n",
    "y_test = y_[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.027296849604358495\n",
      "0.6669255765436917\n",
      "loss 0.028967292758656928\n",
      "0.6617377239364969\n",
      "loss 0.028176301968283562\n",
      "0.6697776038402011\n",
      "loss 0.025090629075071223\n",
      "0.6721837370899266\n",
      "loss 0.025526745046052582\n",
      "0.6697707296297747\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "for epoch in range(5):\n",
    "    running_loss, loss = 0, 0\n",
    "    # training\n",
    "    for i in range(0,len(y),batch):\n",
    "        target = torch.tensor([i for i in y[i:i+batch]],dtype=torch.double).to(device)\n",
    "        inp = pad_sequence([torch.tensor(i,dtype=torch.long) for i in x[i:i+batch]]).to(device)\n",
    "        \n",
    "        h = model.init_hidden(len(target)).to(device)\n",
    "        out = model(inp,h)\n",
    "        \n",
    "        loss = loss_function(out[:,-1,:],target)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 50)\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss\", running_loss/len(y))\n",
    "    \n",
    "    #dev set\n",
    "    batch_ = 1\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i in range(0,len(y_test),batch_):\n",
    "            target = torch.tensor(y_test[i],dtype=torch.double).to(device)\n",
    "            inp = torch.tensor(x_test[i],dtype=torch.long).view(-1,1).to(device)\n",
    "\n",
    "            h = model.init_hidden(len(target)).to(device)\n",
    "            out = model(inp,h)\n",
    "\n",
    "            loss += loss_function(out[:,-1,:],target).item()\n",
    "        print(loss/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning tehcniques does not work better\n",
    "# two reasons being the size of data is small and the length of sente4nces are too big for SOTA NLP techniques\n",
    "# Bert cuts the length to 512 tokens. therefore did not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test.csv\",engine='python')\n",
    "\n",
    "data = data.drop([\"Host\", \"Link\", \"Date(ET)\", \"Time(ET)\", \"time(GMT)\", \"Unnamed: 9\", \"Index\"], axis=1)\n",
    "data.replace(to_replace='FACEBOOK', value='Facebook',inplace=True)\n",
    "replace_ = {}\n",
    "for index, i in enumerate(list(set(data[\"Source\"])),start=1):\n",
    "    replace_[index] = i\n",
    "    data.replace(to_replace=i, value=index,inplace=True)\n",
    "data.fillna('UNK',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Baby Slice, the son of the late Kimbo Slice, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>My Story --</td>\n",
       "      <td>&lt;p&gt;&lt;font face=\"sans-serif\" size=\"3\"&gt;I have had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US FDA ?????canagliflozin?dapagliflozin?????????</td>\n",
       "      <td>Previously, sodium-glucose cotransporter-2 (SG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Hello. I suffer from congestive heart failure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Vitamin D improves heart function, study finds</td>\n",
       "      <td>A daily dose of vitamin D3 improves heart func...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                                             Title  \\\n",
       "0       3                                               UNK   \n",
       "1       2                                       My Story --   \n",
       "2       3  US FDA ?????canagliflozin?dapagliflozin?????????   \n",
       "3       2                                               UNK   \n",
       "4       3    Vitamin D improves heart function, study finds   \n",
       "\n",
       "                                     TRANS_CONV_TEXT  \n",
       "0  Baby Slice, the son of the late Kimbo Slice, h...  \n",
       "1  <p><font face=\"sans-serif\" size=\"3\">I have had...  \n",
       "2  Previously, sodium-glucose cotransporter-2 (SG...  \n",
       "3  Hello. I suffer from congestive heart failure ...  \n",
       "4  A daily dose of vitamin D3 improves heart func...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "for i in sent:\n",
    "    dummy = []\n",
    "    for j in i.split(' '):\n",
    "        try: \n",
    "            if j:\n",
    "                dummy.append(stoi[j])\n",
    "        except: \n",
    "            if j:\n",
    "                dummy.append(stoi['unk'])\n",
    "    inp.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 571/571 [00:10<00:00, 56.05it/s]\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "m = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(inp):\n",
    "        input = torch.tensor(i).view(-1,1).to(device)\n",
    "        h = model.init_hidden(1).to(device)\n",
    "        try:\n",
    "            out_ = model(input,h)\n",
    "            out.append(out_[:,-1,:].item())\n",
    "        except: out.append(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in out:\n",
    "    if i <= 0.25:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = 'Index,Patient_Tag\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(predictions,start=1):\n",
    "    format+=str(index)+','+str(i) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_7.csv\", \"w\") as f:\n",
    "    f.write(format.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
