{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test.csv\",engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum = data.columns\n",
    "for i in colum:\n",
    "    print(f'{len(set(data[i]))} different values in the {i} column')\n",
    "print(f\"\\ntotal number of examples {len(data)}\")\n",
    "\n",
    "#Host, link, Time(ET), Time(GMT),is of no use for trainig the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop([\"Host\", \"Link\", \"Date(ET)\", \"Time(ET)\", \"time(GMT)\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum = data.columns\n",
    "for i in colum:\n",
    "    print(f'{len(set(data[i]))} different values in the {i} column')\n",
    "print(f\"\\ntotal number of examples {len(data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data[\"Source\"])) # differnet values in \"Source\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repalcing FACEBOOK to Facebook \n",
    "data.replace(to_replace='FACEBOOK', value='Facebook',inplace=True)\n",
    "# Now there are only 4 different values in \"Source\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(data.loc[:,\"Source\"]) #  # distribution of differnet values in \"Source\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(data.iloc[:,[-1]]['Patient_Tag']) # distribution of labels in the \"Patien_Tag\" column\n",
    "# It's an unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy = {}\n",
    "for i in list(set(data[\"Source\"])):\n",
    "    print(i,\"---\", Counter(data.iloc[:,[0,-1]][data['Source'] == i]['Patient_Tag']))\n",
    "# distribution of labels with reference to each values in \"Source\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replace_ = {}\n",
    "for index, i in enumerate(list(set(data[\"Source\"])),start=1):\n",
    "    replace_[index] = i\n",
    "    data.replace(to_replace=i, value=index,inplace=True)\n",
    "data.fillna('UNK',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data[\"Source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('UNK',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_with = ['.', '?', '/', '\\n', '(', ')','[', ']', '{', '}', '-','\"','!', '|' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_(sent):\n",
    "    for i in rep_with:\n",
    "        sent = sent.replace(i,' ').replace('$', ' ').replace(',','').replace(\"'\",'')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import num2words\n",
    "\n",
    "def n2w(text):\n",
    "    return re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,pos):\n",
    "    sent = []\n",
    "    for i in range(len(data)):\n",
    "        try:sent.append(n2w(rep_(data.iloc[i,pos])))\n",
    "        except:print(data.iloc[i,pos])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = preprocess(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "sent_preprocess = []\n",
    "for i in sent:\n",
    "    sent_preprocess.append([w for w in word_tokenize(i) if not w in stop_words])\n",
    "sent = sent_preprocess\n",
    "# sent = [i.split(' ') for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = [[sent[i], data.loc[:,'Patient_Tag'][i]] for i in range(len(sent))]\n",
    "train = sorted(train, key = lambda c: len(c[0])) # sorted train using the len of x {helps in minibatching}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = []  # len of each example\n",
    "for i in train:\n",
    "    train_len.append(len(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "try: \n",
    "    with open(\"elmo_st.pkl\", \"rb\") as f:\n",
    "        print(\"loading embeddings\")\n",
    "        embeddings = pickle.load(f)\n",
    "    \n",
    "except:\n",
    "    print(\"creating embeddings ...it will take time\")\n",
    "    from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "    elmo = ElmoEmbedder(\n",
    "    options_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', \n",
    "    weight_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5',\n",
    "    cuda_device=1)\n",
    "\n",
    "    #define max token length, 2187 is the max\n",
    "    max_tokens=1024\n",
    "    #input sentences\n",
    "    sentences = []\n",
    "#     x = [i[0] for i in train]\n",
    "    for i in range(0,len(sent),16):\n",
    "        sentences.append(sent[i:i+16])\n",
    "    #create a pretrained elmo model (requires internet connection)\n",
    "    embeddings=[]\n",
    "\n",
    "    #loop through the input sentences\n",
    "    for k,j in enumerate(sentences):\n",
    "        print(k)\n",
    "        for i, elmo_embedding in enumerate(elmo.embed_sentences(j)): \n",
    "            # Average the 3 layers returned from Elmo\n",
    "            avg_elmo_embedding = np.average(elmo_embedding, axis=0)\n",
    "            padding_length = max_tokens - avg_elmo_embedding.shape[0]\n",
    "            if(padding_length>0):\n",
    "                avg_elmo_embedding =np.append(avg_elmo_embedding, np.zeros((padding_length, avg_elmo_embedding.shape[1])), axis=0)\n",
    "            else:\n",
    "                avg_elmo_embedding=avg_elmo_embedding[:max_tokens]\n",
    "            embeddings.append(avg_elmo_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_elmo.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers,directions,bidirectonal,out):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = directions\n",
    "        self.gru = nn.GRU(1024, hidden_size, num_layers,bidirectional=bidirectonal)\n",
    "        self.linear1 = nn.Linear(hidden_size*directions, 32)\n",
    "        self.linear2 = nn.Linear(32, out, bias = True)\n",
    "        self.drop = nn.Dropout(p=0.7, inplace=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        out,h = self.gru(inp,hidden)\n",
    "        out = self.linear1(out)#.view(-1,2,out.shape[2]))\n",
    "        out = self.linear2(self.drop(out))\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        print()\n",
    "        return torch.zeros(self.num_layers*self.directions, batch_size, self.hidden_size, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(hidden_size=256,num_layers=1,directions=1,bidirectonal=False,out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i[1] for i in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = 64\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for i in range(0,len(embeddings),batch):\n",
    "        h = model.init_hidden(len(embeddings[i:i+batch])).to(device)\n",
    "        target = torch.tensor(y[i:i+batch],dtype=torch.double).to(device)\n",
    "        inp = torch.tensor(embeddings[i:i+batch],dtype=torch.double).view(1830,len(embeddings[i:i+batch]),1024).to(device)\n",
    "        out = model(inp,h)\n",
    "        loss = loss_function(out[-1,:,:],target)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 50)\n",
    "        running_loss+=loss.item()\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss\", running_loss/len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
